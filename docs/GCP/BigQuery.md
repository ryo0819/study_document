# Cloud Composer

# 概要
- サーバーレスでスケーラビリティに優れたデータウェアハウス

- 予測分析などを行える

## 詳細
- 要件的に`ANSI SQLクエリ`や`スキーマ変更される予定`などがあれば選出(Automatically detectというスキーマ変更を検出する機能がある)

- 行でなく`列`を処理するように設計されている
    - 列の処理は`安価で高速`に処理できる
    - 行の処理は高額で低速に処理できる

- 許可されている外部データソース
    - GCS
        - ORC
        - Parquet
        - Avro
        - Json
        - CSV
    - Cloud SQL
    - Googleドライブ

- データ読み込み方法
    - Cloud Console
    - bqコマンドライン
    - API
    - クライアントライブラリ
    - オプションのmaxBadRecordsは無視できる不良レコードの最大数として指定できる
        - `デフォルトは0なので全て有効なレコードである必要がある`

- `パーティション`
    - 作成できる範囲としては以下
        - 時間単位の列
        - 取り込み時間
        - 整数範囲
    - `作成できる上限数は4000件`(一日ごとに分けるとすると約10年分)

- ビューを実装することで以下のメリットがある
    - 列レベル・行レベルのセキュリティを実現することができる
        - <span style="color: red; ">元になるテーブルへのアクセスを許可せずに、特定のユーザーやグループとクエリ結果だけを共有できる</span>
    - 特定の列アクセス・行アクセスやフィルタリングが可能になる

- デフォルトでコンテンツ保存時にGoogleで暗号化＆管理
    - 顧客管理したい場合は、CMEKで暗号化を行いCloud KMSで管理を行う

- ユーザーアクションの監査ログは自動的に作成される

- JOINを使用する際には大きいテーブルをJOINの左に、小さいテーブルを右に書くようにする

- キャッシュを有効にするにはオーナークレデンシャルを設定する必要がある

- `BigQuery ML`でSQLクエリを用いて機械学習モデルを作成できる

- 既存テーブル(ストリーミング)に重複がある場合は`ウィンドウ関数`を使用する
    - 自己結合は使わ内容にする(ベストプラクティス)

- Data StudioはBigQueryデータソースのデータをキャッシュするため更新されていない場合がある
    - データの更新頻度で微調整が可能(Data Studioの設定)

- 新しく変換された列を使用して既存テーブルから新規テーブルを作成(簡単で推奨されている)

- CSVファイルのデフォルトエンコーディングはUTF-8
    - エンコーディングを指定しないと正常にロードできてもバイトが一致しないことがある

- クエリがいくつかの値に大きく偏ったキーを処理する場合は遅くなるため、早い段階でフィルタ処理をする

- TABLE_DATE_RABGE関数は期間に対応する複数の日時テーブルにクエリを実行する

- データが破損している時の対応
    - (正常だった場合)DataflowパイプラインからBigQueryにインポートする
    - (エラーだった場合)別のデットレターテーブルにプッシュする

- 必要最小限の情報のみにアクセスできるようにするには
    - Stackdriver Audit Loggingを使用してポリシー違反を特定
    - BigQueryAPIアクセスを承認されたユーザーに制限
    - 複数のテーブルまたはDB間でデータを分離
    - データセットに必要なアクセス制限をかける

- エクスポートの制限事項
    - GCSにのみテーブルデータをエクスポートできる
    - 1ファイル1GBまで、超える場合はワイルドカードを使用して複数ファイルにエクスポートする
    - ネストされたデータ、繰り返しデータは以下の形式でエクスポートされる
        - Json: GZIP
        - Avro: DEFLATE, SNAPPY
        - Parquet: SNAPPY, GZIP
        - (ネスト・繰り返しがなければ)CSV: GZIP
    - コンソールからはGZIP以外の圧縮タイプを選択できない

- 特定の列に対してフィルター・集計を使用するクエリのパフォーマンス向上にはクラスター化されたパーティションテーブルを使う
    - パーティションテーブルが特定の行
    - クラスターが特定の列

- ツールやアプリケーションからのBigQueryにアクセスを許可するにはサービスアカウントが最善策

- テーブルが`90日間連続`で編集されないとストレージ価格が50%に下がる(`一度でも編集すると元通り`)
    - +α: 長期保存でもパフォーマンスなどは低下しない

- データ構造は大きい順から
    - プロジェクト
        - データセット
            - テーブル
                - カラム

- 既存の列の組み合わせから新しい列を作成するには
    - テーブルを上書きする
    - 新しいテーブルを作成する

- 外部データソースを利用する場合
    - 頻繁に変更されるデータを外部データソースに直接読み込むことで、更新の度に取り込まずに済む
    - ETLワークロードの場合、クリーンアップした結果を書き込む

- 必要な情報のみにアクセスできるようにした後の管理・データ保護
    - 各ユーザーに適切なID・IAMの役割を付与
    - データセットを承認されたユーザーに制限
    - 見せたいデータごとに異なるデータセットにデータをロード

## コスト節約で気をつける点
- データの一括読み込みは無料
- プレビューオプションを使用してデータサンプリング
- CLIで`--dry_run`フラグをつける
- 料金計算ツールで見積もる
- 正確な値が必要ない場合は近似集計関数を使用する
- SELECT句で抽出する列を決めることでデータ処理量を減らす
- パーティショニングでスキャン量を減らす
    - 上記2つはパフォーママンスの向上にもつながる

- ひっかけでLIMIT句を使用するというのもあるので`注意`

